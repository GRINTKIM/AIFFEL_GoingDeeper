{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoingDeeper12.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 직접 만들어보는 OCR\n",
        "\n",
        "<br>\n",
        "\n",
        "이번 시간에는 OCR을 직접 만들어보는 시간을 갖도록 하겠습니다. OCR을 처음부터 끝까지 만들기에는 시간이 많이 소요되므로 Detection은 keras-ocr을 활용하고, Recognition은 직접 만들어 학습해보도록 하겠습니다.\n",
        "\n",
        "### 실습목표\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "* Text Recognition 모델을 직접 구현해 봅니다.\n",
        "* Text Recognition 모델 학습을 진행해 봅니다.\n",
        "* Text Detection 모델과 연결하여 전체 OCR 시스템을 구현합니다.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "exJ1WIDsdasN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for OCR\n",
        "\n",
        "<br>\n",
        "\n",
        "OCR은 데이터셋에 필요한 텍스트 정보를 사람이 직접 입력해야 하는 번거로움이 있습니다. 따라서 OCR 데이터를 대량으로 만들려면 큰 비용이 듭니다. 데이터 문제를 해결하기 위한 방법 중 하나로는 컴퓨터로 대량 문자 이미지 데이터를 만들어내는 방법입니다. 직접 문자 데이터를 생성하게 되면, 원하는 언어를 원하는 폰트와 원하는 배치 및 크기로 문자 이미지를 대량으로 만들어낼 수 있다는 장점이 있습니다.\n",
        "\n",
        "이전 스텝에서 소개했던 [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)과 같은 논문들에서는 Recognition model의 정량적인 평가를 위해서 <code>MJSynth</code>와 <code>SynthText</code>라는 데이터셋을 활용합니다. Recognition model을 제안하는 다양한 논문들에서도 성능 비교를 위해 두 데이터를 활용한다는 것을 기억해두시면 좋을 것 같습니다.\n",
        "\n",
        "1. MJSynth\n",
        "\n",
        "2. SynthText\n",
        "\n",
        "앞으로 만들어볼 Recognition model 학습을 위해 <code>MJSynth</code>를 사용해보도록 하겠습니다. 아래의 링크는 Naver Clova의 논문 저자들이 Dropbox를 통해 제공하는 데이터셋입니다.\n",
        "\n",
        "아래 링크의 training 폴더에서 <code>data_lmdb_release.zip</code> 내 있는 MJ 데이터만 활용할 예정입니다.\n",
        "\n",
        "* [Dropbox-data_lmdb_release](https://www.dropbox.com/sh/i39abvnefllx2si/AAAbAYRvxzRp3cIE5HzqUw3ra?dl=0)\n",
        "\n",
        "클라우드를 사용중이라면 다운로드는 받지 않으셔도 됩니다.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### 데이터 준비"
      ],
      "metadata": {
        "id": "AJQioxaTzUW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = os.path.join(os.getenv('HOME'),'aiffel/ocr')\n",
        "os.chdir(path)\n",
        "\n",
        "print(path)"
      ],
      "metadata": {
        "id": "-ZhImphd1F1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (1)\n",
        "\n",
        "<br>\n",
        "\n",
        "Text recognition 모델을 직접 만들어보도록 하겠습니다. Recognition 모델은 2015년에 발표된 아래 논문에서 처음 소개된 CRNN 구조를 활용해서 만들어보도록 합시다.\n",
        "\n",
        "<br>\n",
        "\n",
        "* [An End-to-End Trainable Neural Network for Image-based SequenceRecognition and Its Application to Scene Text Recognition](https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "<br>\n",
        "\n",
        "[![](https://d3s0tskafalll9.cloudfront.net/media/original_images/e-23-2.crnn.png)](https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "CRNN의 구조는 위 그림에서 아래부터 올라가는 순서로 보시면 됩니다. 입력이미지를 Convolution Layer를 통해 Feature를 추출하여 추출된 Feature를 얻어냅니다. Recurrent Layer는 추출된 Feature의 전체적인 Context를 파악하고 다양한 output의 크기에 대응이 가능합니다. 끝으로 Transcription layer(Fully connected layer)는 step마다 어떤 character의 확률이 높은지 예측합니다. 아래의 표를 통해 정확한 구조를 확인해주세요.\n",
        "\n",
        "[![](https://d3s0tskafalll9.cloudfront.net/media/original_images/e-23-3.crnn_structure.png)](https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "<br>\n",
        "\n",
        "몇 개의 Class가 필요한지 확인하기 위해 다음 셀을 실행한 후 퀴즈를 풀어보세요.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "h4XNGx0L1Mse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBERS = \"0123456789\"\n",
        "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
        "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
      ],
      "metadata": {
        "id": "T9VdqzRn2ret"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall structure of OCR\n",
        "\n",
        "<br>\n",
        "\n",
        "[![](https://d3s0tskafalll9.cloudfront.net/media/images/e-23-1.ocr-system.max-800x600.png)](https://brunch.co.kr/@kakao-it/318)\n",
        "\n",
        "<br>\n",
        "\n",
        "우리가 만들고자 하는 OCR은 이미지 속에서 영문을 Bounding box로 찾아내고 그 Bounding box 내에 어떤 Text가 포함되는지 알 수 있는 시스템입니다. 이미지 속에서 문자 영역을 찾아내는 것인 **Text Detection**은 이전에 봤던 방법 중 Segmentation 기반의 **CRAFT**를 활용한 **keras-ocr**을 활용할 예정입니다. **Recognition** 모델은 keras-ocr을 사용하지 않고 직접 만들어보도록 하겠습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "* keras-ocr 공식 github\n",
        "\n",
        "* CRAFT: Character Region Awareness for Text detection\n",
        "\n",
        " * CRAFT Pytorch 공식 implementation\n",
        "\n",
        " * CRAFT Keras 버전 github\n",
        "\n",
        "<br>\n",
        "\n",
        ">keras-ocr에서도 recognition을 지원하는데요 이 모델은 어떤 구조를 썼을까요?\n",
        "\n",
        ">Convolution layer와 RNN을 결합하고 CTC로 학습된 CRNN이 사용되었습니다.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "CNN과 RNN의 아이디어를 결합하여 Text Recognition의 초기 모델의 뼈대를 완성했던 CRNN 모델도 2015년에 나온 것입니다. 그 이후로도 다양한 모델들이 새로운 기법을 제시하며 조금씩 성능 향상을 이루어 왔습니다. 2019년에 발표된 Naver Clova의 아래 논문에서 당시까지의 모델의 발전사를 잘 살펴볼 수 있습니다.\n",
        "\n",
        "* [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)\n",
        "\n",
        "<br>\n",
        "\n",
        ">keras-ocr의 CRNN 기반 Recognition 모델과 위 논문에 소개된 Recognition에서 가장 높은 성능을 얻은 (저자들의) 모델은 어떤 점이 다를까요?\n",
        "\n",
        ">첫 번째로 입력 이미지 변환 단계에서는 모델의 앞에서 글자를 Thin plate spline Transformation을 해주는 TPS 모듈이 붙고, 마지막 Text 출력 단계에서는 Bidirectional LSTM 뒤로 Attention decoder가 붙습니다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "DHYkL4toxlGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">영문 대문자와 숫자를 인식하기 위해서는 몇가지의 Class가 필요할까요?\n",
        "\n",
        ">총 36가지가 필요합니다. 문자가 없는 경우를 위해서 공백을 추가할 경우 class의 수는 37개가 됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "-fOaSCpt2wKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 lmdb를 이용할 예정입니다. LMDB는 Symas에서 만든 Lightning Memory-Mapped Database의 약자입니다. 오늘 우리가 다루게 될 데이터셋이 lmdb 포맷(mdb)의 파일로 이루어져 있습니다.\n",
        "\n",
        "클라우드에는 이미 설치되어 있으므로 아래의 명령어는 참고로 알아두세요.\n",
        "\n",
        "<br>\n",
        "\n",
        "<pre><code>$ pip install lmdb</code></pre>\n",
        "\n",
        "<br>\n",
        "\n",
        "필요한 라이브러리들을 import합니다. 다운로드한 MJ 데이터셋의 위치도 확인해 주세요!\n",
        "\n"
      ],
      "metadata": {
        "id": "sndbWxH124Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import six\n",
        "import math\n",
        "import lmdb\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
        "\n",
        "TRAIN_DATA_PATH = HOME_DIR+'/data/MJ/MJ_train'\n",
        "VALID_DATA_PATH = HOME_DIR+'/data/MJ/MJ_valid'\n",
        "TEST_DATA_PATH = HOME_DIR+'/data/MJ/MJ_test'\n",
        "\n",
        "print(TRAIN_DATA_PATH)"
      ],
      "metadata": {
        "id": "o4jSyiel5dRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (2) Input Image\n",
        "\n",
        "<br>\n",
        "\n",
        "데이터셋 안에 들어있는 이미지가 실제로 어떻게 생겼는지 확인해 봅시다. 아래의 코드를 실행해 lmdb를 통해 훈련데이터셋의 이미지를 4개만 열어서 실제 shape가 어떻게 생겼는지, 이미지나 라벨은 어떻게 달려 있는지를 확인해 보도록 합시다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "h94294-U5iZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# env에 데이터를 불러올게요\n",
        "# lmdb에서 데이터를 불러올 때 env라는 변수명을 사용하는게 일반적이에요\n",
        "env = lmdb.open(TRAIN_DATA_PATH, \n",
        "                max_readers=32, \n",
        "                readonly=True, \n",
        "                lock=False, \n",
        "                readahead=False, \n",
        "                meminit=False)\n",
        "\n",
        "# 불러온 데이터를 txn(transaction)이라는 변수를 통해 엽니다\n",
        "# 이제 txn변수를 통해 직접 데이터에 접근 할 수 있어요\n",
        "with env.begin(write=False) as txn:\n",
        "    for index in range(1, 5):\n",
        "        # index를 이용해서 라벨 키와 이미지 키를 만들면\n",
        "        # txn에서 라벨과 이미지를 읽어올 수 있어요\n",
        "        label_key = 'label-%09d'.encode() % index\n",
        "        label = txn.get(label_key).decode('utf-8')\n",
        "        img_key = 'image-%09d'.encode() % index\n",
        "        imgbuf = txn.get(img_key)\n",
        "        buf = six.BytesIO()\n",
        "        buf.write(imgbuf)\n",
        "        buf.seek(0)\n",
        "\n",
        "        # 이미지는 버퍼를 통해 읽어오기 때문에 \n",
        "        # 버퍼에서 이미지로 변환하는 과정이 다시 필요해요\n",
        "        try:\n",
        "            img = Image.open(buf).convert('RGB')\n",
        "\n",
        "        except IOError:\n",
        "            img = Image.new('RGB', (100, 32))\n",
        "            label = '-'\n",
        "\n",
        "        # 원본 이미지 크기를 출력해 봅니다\n",
        "        width, height = img.size\n",
        "        print('original image width:{}, height:{}'.format(width, height))\n",
        "        \n",
        "        # 이미지 비율을 유지하면서 높이를 32로 바꿀거에요\n",
        "        # 하지만 너비를 100보다는 작게하고 싶어요\n",
        "        target_width = min(int(width*32/height), 100)\n",
        "        target_img_size = (target_width,32)        \n",
        "        print('target_img_size:{}'.format(target_img_size))        \n",
        "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
        "\n",
        "        # 이제 높이가 32로 일정한 이미지와 라벨을 함께 출력할 수 있어요       \n",
        "        print('display img shape:{}'.format(img.shape))\n",
        "        print('label:{}'.format(label))\n",
        "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
      ],
      "metadata": {
        "id": "F0d3cD3o5ohT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어떤가요? 대부분의 이미지의 height는 31, 최대 32까지로 되어 있고, width는 문자열 길이에 따라 다양한 것으로 보입니다.\n",
        "\n",
        "이제부터 lmdb를 활용하여 케라스 모델 학습용 <code>MJSynth</code>데이터셋 클래스를 구현하려고 합니다. <code>dataset_path</code>는 읽어들일 데이터셋의 경로입니다. <code>label_converter</code>는 아래에서 여러분이 문자를 미리정의된 index로 변환해주는 converter로 직접 구현하도록 합니다. 이외에도 <code>batch_size</code>와 입력이미지 크기 그리고 필터링을 위한 최대 글자 수, 학습대상으로 한정하기 위한 character등을 입력으로 받도록 구현되어 있습니다.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "mRqg8lDK5sdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MJDatasetSequence(Sequence):\n",
        "    # 객체를 초기화 할 때 lmdb를 열어 env에 준비해둡니다\n",
        "    # 또, lmdb에 있는 데이터 수를 미리 파악해둡니다\n",
        "    def __init__(self, \n",
        "                 dataset_path,\n",
        "                 label_converter,\n",
        "                 batch_size=1,\n",
        "                 img_size=(100,32),\n",
        "                 max_text_len=22,\n",
        "                 is_train=False,\n",
        "                 character='') :\n",
        "        \n",
        "        self.label_converter = label_converter\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.max_text_len = max_text_len\n",
        "        self.character = character\n",
        "        self.is_train = is_train\n",
        "        self.divide_length = 100\n",
        "\n",
        "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            self.num_samples = int(txn.get('num-samples'.encode()))\n",
        "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
        "    \n",
        "    # index에 해당하는 image와 label을 읽어옵니다\n",
        "    # 위에서 사용한 코드와 매우 유사합니다\n",
        "    # label을 조금 더 다듬는 것이 약간 다릅니다\n",
        "    def _get_img_label(self, index):\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            label_key = 'label-%09d'.encode() % index\n",
        "            label = txn.get(label_key).decode('utf-8')\n",
        "            img_key = 'image-%09d'.encode() % index\n",
        "            imgbuf = txn.get(img_key)\n",
        "\n",
        "            buf = six.BytesIO()\n",
        "            buf.write(imgbuf)\n",
        "            buf.seek(0)\n",
        "            try:\n",
        "                img = Image.open(buf).convert('RGB')\n",
        "\n",
        "            except IOError:\n",
        "                img = Image.new('RGB', self.img_size)\n",
        "                label = '-'\n",
        "            width, height = img.size\n",
        "            \n",
        "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
        "            target_img_size = (target_width, self.img_size[1])\n",
        "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
        "            # label을 약간 더 다듬습니다\n",
        "            label = label.upper()\n",
        "            out_of_char = f'[^{self.character}]'\n",
        "            label = re.sub(out_of_char, '', label)\n",
        "            label = label[:self.max_text_len]\n",
        "\n",
        "        return (img, label)\n",
        "    \n",
        "    # __getitem__은 약속되어있는 메서드입니다\n",
        "    # 이 부분을 작성하면 slice할 수 있습니다\n",
        "    # 자세히 알고 싶다면 아래 문서를 참고하세요\n",
        "    # https://docs.python.org/3/reference/datamodel.html#object.__getitem__\n",
        "    # \n",
        "    # 1. idx에 해당하는 index_list만큼 데이터를 불러\n",
        "    # 2. image와 label을 불러오고 \n",
        "    # 3. 사용하기 좋은 inputs과 outputs형태로 반환합니다\n",
        "    def __getitem__(self, idx):\n",
        "        # 1.\n",
        "        batch_indicies = self.index_list[\n",
        "            idx*self.batch_size:\n",
        "            (idx+1)*self.batch_size\n",
        "        ]\n",
        "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
        "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
        "\n",
        "        input_length = np.ones([self.batch_size], dtype='int64') * self.max_text_len\n",
        "        label_length = np.ones([self.batch_size], dtype='int64')\n",
        "\n",
        "        # 2.\n",
        "        for i, index in enumerate(batch_indicies):\n",
        "            img, label = self._get_img_label(index)\n",
        "            encoded_label = self.label_converter.encode(label)\n",
        "            # 인코딩 과정에서 '-'이 추가되면 max_text_len보다 길어질 수 있어요\n",
        "            if len(encoded_label) > self.max_text_len:\n",
        "                continue\n",
        "            width = img.shape[0]\n",
        "            input_images[i,:width,:,:] = img\n",
        "            labels[i,0:len(encoded_label)] = encoded_label\n",
        "            label_length[i] = len(encoded_label)\n",
        "        \n",
        "        # 3.\n",
        "        inputs = {\n",
        "            'input_image': input_images,\n",
        "            'label': labels,\n",
        "            'input_length': input_length,\n",
        "            'label_length': label_length,\n",
        "        }\n",
        "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
        "\n",
        "        return inputs, outputs"
      ],
      "metadata": {
        "id": "C00QKX245v1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 분석코드처럼 이미지 데이터를 img, label의 쌍으로 가져오는 부분은 <code>_get_img_label()</code> 메소드에 반영되었습니다. 그리고 <code>model.fit()</code>에서 호출되는 <code>__getitem__()</code> 메소드에서 배치 단위만큼 <code>_get_img_label()</code> 를 통해 가져온 데이터셋을 리턴하게 될 것입니다. <code>_get_img_label()</code> 를 보면 다양한 사이즈의 이미지를 모두 height는 32로 맞추고, width는 최대 100까지로 맞추게끔 가공하고 있습니다.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "WJiEuWGT6KkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (3) Encode\n",
        "\n",
        "<br>\n",
        "\n",
        "이전 스텝에서 살펴본 바에 의하면, Label이 우리가 읽을 수 있는 평문 Text로 이루어져 있었습니다. 그러나 이것은 모델을 학습하기 위해서 적절한 형태가 아닙니다. 따라서 각 Character를 class로 생각하고 이를 step에 따른 class index로 변환해서 encode를 해주어야 합니다. 이를 해줄 수 있는 <code>LabelConverter</code> 클래스를 작성해 봅시다.\n",
        "\n",
        "* <code>__init__()</code>에서는 입력으로 받은 text를 <code>self.dict</code>에 각 character들이 어떤 index에 매핑되는지 저장합니다. 이 character와 index 정보를 통해 모델이 학습할 수 있는 output이 만들어집니다. 만약 <code>character='ABCD'</code>라면 <code>'A'</code>의 label은 1, <code>'B'</code>의 label은 2가 됩니다.\n",
        "* 공백(blank) 문자를 지정합니다. 여기서는 공백 문자를 뜻하기 위해 <code>'-'</code>를 활용하며, label은 0으로 지정합니다.\n",
        "* <code>decode()</code>는 각 index를 다시 character로 변환한 후 이어주어 우리가 읽을 수 있는 text로 바꾸어줍니다.\n",
        "\n",
        ">입력받은 text를 모델이 학습할 수 있는 label로 만드는 encode() 메소드를 구현해 주세요!\n",
        "단, 같은 글자가 연속으로 이어지는 경우에는 이어지는 그 사이에 공백 문자의 label을 포함해야 합니다!\n",
        "\n",
        "OCR 모델 학습데이터에 왜 공백 문자가 포함되어야 하는지는 다음 스텝에서 설명할 예정입니다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "UkP9oh_m6ikU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelConverter(object):\n",
        "\n",
        "     def __init__(self, character):\n",
        "         self.character = \"-\" + character\n",
        "         self.label_map = dict()\n",
        "         for i, char in enumerate(self.character):\n",
        "             self.label_map[char] = i\n",
        "\n",
        "     def encode(self, text):\n",
        "         encoded_label = []\n",
        "         for i, char in enumerate(text):\n",
        "             if i > 0 and char == text[i - 1]:\n",
        "                 encoded_label.append(0)    # 같은 문자 사이에 공백 문자 label을 삽입\n",
        "             encoded_label.append(self.label_map[char])\n",
        "         return np.array(encoded_label)\n",
        "\n",
        "     def decode(self, encoded_label):\n",
        "         target_characters = list(self.character)\n",
        "         decoded_label = \"\"\n",
        "         for encode in encoded_label:\n",
        "             decoded_label += self.character[encode]\n",
        "         return decoded_label"
      ],
      "metadata": {
        "id": "ZbzNa4nR7Qm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래에서 'HELLO'를 Encode한 후 Decode가 정상적으로 되는지 확인해보도록 해보세요!"
      ],
      "metadata": {
        "id": "bphJS2eP7cyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
        "\n",
        "encdoded_text = label_converter.encode('HELLO')\n",
        "print(\"Encdoded_text: \", encdoded_text)\n",
        "decoded_text = label_converter.decode(encdoded_text)\n",
        "print(\"Decoded_text: \", decoded_text)"
      ],
      "metadata": {
        "id": "XwRWQJ7J7grs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "동일한 글자 <code>'L'</code>이 연속될 때, 그 사이에 공백 문자가 포함된 것을 확인할 수 있습니다.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "QnIdoo2P7inJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (4) Build CRNN model\n",
        "\n",
        "<br>\n",
        "\n",
        "이제 입력과 출력을 준비했으니 모델을 만들어볼 차례입니다. Keras에서 제공하는 <code>K.ctc_batch_cost()</code>를 활용해서 loss를 계산하도록 <code>ctc_lambda_func</code>를 아래와 같이 만들어두었습니다."
      ],
      "metadata": {
        "id": "DEQ7z0OO7oHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_lambda_func(args): # CTC loss를 계산하기 위한 Lambda 함수\n",
        "    labels, y_pred, label_length, input_length = args\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "metadata": {
        "id": "XEcy_93AFnA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 잠시 Keras의 <code>K.ctc_batch_cost()</code> 함수에 대해 짚고 넘어갑시다. 비록 우리가 이 함수 내부를 직접 구현하진 않겠지만 CTC Loss 함수를 구현하기 위해 우리가 이 함수에 인자로 어떤 값을 넘겨야 하는지는 명확하게 파악해야 합니다.\n",
        "\n",
        "* [Tensorflow Tutorial - ctc_batch_cost](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/backend/ctc_batch_cost)\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/ctc.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "우리는 CTC Loss를 활용해야 하는 모델이 위 그림과 같은 상황을 다루기 위한 것임을 알고 있습니다. 입력의 길이 T와 라벨의 길이 U의 단위가 일치하지 않을 때, 그래서 라벨은 <code>APPLE</code>이지만 모델이 출력한 결과는 <code>AAAPPPPLLLLEE</code> 처럼 나올 수 있습니다. 이런 상황이 이미지에서 텍스트 라벨을 추론해야 하는 Text recognition 태스크에 동일하게 적용됩니다.\n",
        "\n",
        "<br>\n",
        "\n",
        ">만약 모델이 AAAPPPPLLLLEE을 출력했다고 합시다. 이때 추론 결과는 APLE일지 APPLE일지 구분이 가능할까요? 이 경우에는 APLE로 결론을 내리게 될 것입니다.\n",
        ">\n",
        ">그러므로 추론 결과가 APPLE이 되게 하려면 이미지의 라벨은 AP-PLE로 보정해 주어야 합니다. 그래서 모델이 AAAPP-PPLLLEE로 출력을 한다면 추론 결과는 APPLE이 되는 것입니다. 이런 이유로 이전 스텝에서 LabelConverter.encode() 메소드에 공백문자 처리로직을 포함했던 것입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "위 텐서플로우 튜토리얼에 따르면, <code>K.ctc_batch_cost(y_true, y_pred, input_length, label_length)</code>에는 4가지 인자가 존재합니다. 각각의 인자의 의미는 다음과 같습니다.\n",
        "\n",
        "* y_true: tensor (samples, max_string_length) containing the truth labels.\n",
        "* y_pred: tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.\n",
        "* input_length tensor: (samples, 1) containing the sequence length for each batch item in y_pred.\n",
        "* label_length tensor: (samples, 1) containing the sequence length for each batch item in y_true.\n",
        "\n",
        "(여기서 samples는 배치사이즈를 의미합니다.)\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-6-P-example.png)\n",
        "\n",
        "위 그림은 이전 스텝에서 살펴본 실제 데이터셋 예시입니다. 이 케이스를 예로 들었을 때 위 인자들은 다음과 같이 될 것입니다.\n",
        "\n",
        "* <code>y_true</code>: 실제 라벨 <code>LUBE</code>. 텍스트 라벨 그대로가 아니라, 각 글자를 One-hot 인코딩한 형태로서, max_string_length 값은 모델에서 22로 지정할 예정\n",
        "* <code>y_pred</code>: 우리가 만들 RCNN <code>모델의 출력 결과</code>. 길이는 4가 아니라 우리가 만들 RNN의 최종 출력 길이로서 24가 될 예정\n",
        "* <code>input_length tensor</code>: 모델 입력 길이 T로서, 이 경우에는 텍스트의 width인 <code>74</code>\n",
        "* <code>label_length tensor</code>: 라벨의 실제 정답 길이 U로서, 이 경우에는 <code>4</code>\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "이제, <code>K.ctc_batch_cost()</code>를 활용하여, <code>image_input</code>을 입력으로, 마지막 Label을 'output'이라는 이름으로 출력하는 레이어를 갖도록 모델을 만드는 함수 <code>build_crnn_model()</code>을 구현해 봅시다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "JEcDGZ-7Foje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
        "    num_chars = len(characters)+2\n",
        "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
        "    \n",
        "    # Build CRNN model\n",
        "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
        "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
        "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.BatchNormalization()(conv)\n",
        "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.BatchNormalization()(conv)\n",
        "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
        "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
        "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
        "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
        "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
        "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
        "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
        "\n",
        "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
        "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
        "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
        "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
        "        [labels, y_pred, label_length, input_length]\n",
        "    )\n",
        "    model_input = [image_input, labels, input_length, label_length]\n",
        "    model = Model(\n",
        "        inputs=model_input,\n",
        "        outputs=loss_out\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "a9X1FpuDH8uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (5) Train & Inference\n",
        "\n",
        "<br>\n",
        "\n",
        "이제 앞에서 정의한 <code>MJDatasetSequence</code>로 데이터를 적절히 분리하여 구성된 데이터셋을 통해 학습을 시켜봅시다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "B_C67gcJIA9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋과 모델을 준비합니다\n",
        "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
        "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
        "model = build_crnn_model()\n",
        "\n",
        "# 모델을 컴파일 합니다\n",
        "optimizer = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "_QGHVizHIJRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제 학습을 위해선 많은 시간이 소요되니 여기서는 예시로 1 Epoch만 돌려보겠습니다. 뒤에서는 20Epoch 이상 학습된 모델의 가중치를 불러와서 진행할게요.😘\n",
        "\n",
        "EarlyStopping을 이용하면 훈련이 더 빨리 끝날 수도 있어요!\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "VBbJFusnILQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련이 빨리 끝날 수 있도록 ModelCheckPoint와 EarlyStopping을 사용합니다\n",
        "checkpoint_path = HOME_DIR + '/model_checkpoint.hdf5'\n",
        "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_loss',\n",
        "    verbose=1, save_best_only=True, save_weights_only=True\n",
        ")\n",
        "earlystop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='min'\n",
        ")\n",
        "model.fit(train_set,\n",
        "          steps_per_epoch=len(train_set),\n",
        "          epochs=1,\n",
        "          validation_data=val_set,\n",
        "          validation_steps=len(val_set),\n",
        "          callbacks=[ckp, earlystop])"
      ],
      "metadata": {
        "id": "iWKDRb2MIOW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 학습된 모델을 테스트셋을 통해 확인해 볼 차례입니다."
      ],
      "metadata": {
        "id": "Mt7Xe1a8IQr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음은 학습된 모델의 가중치가 저장된 경로입니다\n",
        "checkpoint_path = HOME_DIR + '/data/model_checkpoint.hdf5'\n",
        "\n",
        "# 데이터셋과 모델을 불러옵니다\n",
        "test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
        "model = build_crnn_model()\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# crnn 모델은 입력이 복잡한 구조이므로 그대로 사용할 수가 없습니다\n",
        "# 그래서 crnn 모델의 입력중 'input_image' 부분만 사용한 모델을 새로 만들겁니다\n",
        "# inference 전용 모델이에요 \n",
        "input_data = model.get_layer('input_image').output\n",
        "y_pred = model.get_layer('output').output\n",
        "model_pred = Model(inputs=input_data, outputs=y_pred)"
      ],
      "metadata": {
        "id": "6AgG0ymBIVJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 모델의 성능을 눈으로 확인해봅시다."
      ],
      "metadata": {
        "id": "V3Uhh6Q9Ii0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# 모델이 inference한 결과를 글자로 바꿔주는 역할을 합니다\n",
        "# 코드 하나하나를 이해하기는 조금 어려울 수 있습니다\n",
        "def decode_predict_ctc(out, chars = TARGET_CHARACTERS):\n",
        "    results = []\n",
        "    indexes = K.get_value(\n",
        "        K.ctc_decode(\n",
        "            out, input_length=np.ones(out.shape[0]) * out.shape[1],\n",
        "            greedy=False , beam_width=5, top_paths=1\n",
        "        )[0][0]\n",
        "    )[0]\n",
        "    text = \"\"\n",
        "    for index in indexes:\n",
        "        text += chars[index]\n",
        "    results.append(text)\n",
        "    return results\n",
        "\n",
        "# 모델과 데이터셋이 주어지면 inference를 수행합니다\n",
        "# index개 만큼의 데이터를 읽어 모델로 inference를 수행하고\n",
        "# 결과를 디코딩해 출력해줍니다\n",
        "def check_inference(model, dataset, index = 5):\n",
        "    for i in range(index):\n",
        "        inputs, outputs = dataset[i]\n",
        "        img = dataset[i][0]['input_image'][0:1,:,:,:]\n",
        "        output = model.predict(img)\n",
        "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
        "        print(\"Result: \\t\", result)\n",
        "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))\n",
        "\n",
        "check_inference(model_pred, test_set, index=10)"
      ],
      "metadata": {
        "id": "kmpSOgvfItRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: End-to-End OCR"
      ],
      "metadata": {
        "id": "RgoEEc2HpURT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CeVQ-waepW4t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}