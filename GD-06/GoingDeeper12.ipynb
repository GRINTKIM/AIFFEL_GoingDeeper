{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoingDeeper12.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ì§ì ‘ ë§Œë“¤ì–´ë³´ëŠ” OCR\n",
        "\n",
        "<br>\n",
        "\n",
        "ì´ë²ˆ ì‹œê°„ì—ëŠ” OCRì„ ì§ì ‘ ë§Œë“¤ì–´ë³´ëŠ” ì‹œê°„ì„ ê°–ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. OCRì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë§Œë“¤ê¸°ì—ëŠ” ì‹œê°„ì´ ë§ì´ ì†Œìš”ë˜ë¯€ë¡œ Detectionì€ keras-ocrì„ í™œìš©í•˜ê³ , Recognitionì€ ì§ì ‘ ë§Œë“¤ì–´ í•™ìŠµí•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ì‹¤ìŠµëª©í‘œ\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "* Text Recognition ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.\n",
        "* Text Recognition ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•´ ë´…ë‹ˆë‹¤.\n",
        "* Text Detection ëª¨ë¸ê³¼ ì—°ê²°í•˜ì—¬ ì „ì²´ OCR ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "exJ1WIDsdasN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for OCR\n",
        "\n",
        "<br>\n",
        "\n",
        "OCRì€ ë°ì´í„°ì…‹ì— í•„ìš”í•œ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì‚¬ëŒì´ ì§ì ‘ ì…ë ¥í•´ì•¼ í•˜ëŠ” ë²ˆê±°ë¡œì›€ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ OCR ë°ì´í„°ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ë§Œë“¤ë ¤ë©´ í° ë¹„ìš©ì´ ë“­ë‹ˆë‹¤. ë°ì´í„° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œëŠ” ì»´í“¨í„°ë¡œ ëŒ€ëŸ‰ ë¬¸ì ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì§ì ‘ ë¬¸ì ë°ì´í„°ë¥¼ ìƒì„±í•˜ê²Œ ë˜ë©´, ì›í•˜ëŠ” ì–¸ì–´ë¥¼ ì›í•˜ëŠ” í°íŠ¸ì™€ ì›í•˜ëŠ” ë°°ì¹˜ ë° í¬ê¸°ë¡œ ë¬¸ì ì´ë¯¸ì§€ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ì „ ìŠ¤í…ì—ì„œ ì†Œê°œí–ˆë˜ [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)ê³¼ ê°™ì€ ë…¼ë¬¸ë“¤ì—ì„œëŠ” Recognition modelì˜ ì •ëŸ‰ì ì¸ í‰ê°€ë¥¼ ìœ„í•´ì„œ <code>MJSynth</code>ì™€ <code>SynthText</code>ë¼ëŠ” ë°ì´í„°ì…‹ì„ í™œìš©í•©ë‹ˆë‹¤. Recognition modelì„ ì œì•ˆí•˜ëŠ” ë‹¤ì–‘í•œ ë…¼ë¬¸ë“¤ì—ì„œë„ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ ë‘ ë°ì´í„°ë¥¼ í™œìš©í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ë‘ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "1. MJSynth\n",
        "\n",
        "2. SynthText\n",
        "\n",
        "ì•ìœ¼ë¡œ ë§Œë“¤ì–´ë³¼ Recognition model í•™ìŠµì„ ìœ„í•´ <code>MJSynth</code>ë¥¼ ì‚¬ìš©í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ë§í¬ëŠ” Naver Clovaì˜ ë…¼ë¬¸ ì €ìë“¤ì´ Dropboxë¥¼ í†µí•´ ì œê³µí•˜ëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ ë§í¬ì˜ training í´ë”ì—ì„œ <code>data_lmdb_release.zip</code> ë‚´ ìˆëŠ” MJ ë°ì´í„°ë§Œ í™œìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "* [Dropbox-data_lmdb_release](https://www.dropbox.com/sh/i39abvnefllx2si/AAAbAYRvxzRp3cIE5HzqUw3ra?dl=0)\n",
        "\n",
        "í´ë¼ìš°ë“œë¥¼ ì‚¬ìš©ì¤‘ì´ë¼ë©´ ë‹¤ìš´ë¡œë“œëŠ” ë°›ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### ë°ì´í„° ì¤€ë¹„"
      ],
      "metadata": {
        "id": "AJQioxaTzUW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = os.path.join(os.getenv('HOME'),'aiffel/ocr')\n",
        "os.chdir(path)\n",
        "\n",
        "print(path)"
      ],
      "metadata": {
        "id": "-ZhImphd1F1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (1)\n",
        "\n",
        "<br>\n",
        "\n",
        "Text recognition ëª¨ë¸ì„ ì§ì ‘ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Recognition ëª¨ë¸ì€ 2015ë…„ì— ë°œí‘œëœ ì•„ë˜ ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì†Œê°œëœ CRNN êµ¬ì¡°ë¥¼ í™œìš©í•´ì„œ ë§Œë“¤ì–´ë³´ë„ë¡ í•©ì‹œë‹¤.\n",
        "\n",
        "<br>\n",
        "\n",
        "* [An End-to-End Trainable Neural Network for Image-based SequenceRecognition and Its Application to Scene Text Recognition](https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "<br>\n",
        "\n",
        "[![](https://d3s0tskafalll9.cloudfront.net/media/original_images/e-23-2.crnn.png)](https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "CRNNì˜ êµ¬ì¡°ëŠ” ìœ„ ê·¸ë¦¼ì—ì„œ ì•„ë˜ë¶€í„° ì˜¬ë¼ê°€ëŠ” ìˆœì„œë¡œ ë³´ì‹œë©´ ë©ë‹ˆë‹¤. ì…ë ¥ì´ë¯¸ì§€ë¥¼ Convolution Layerë¥¼ í†µí•´ Featureë¥¼ ì¶”ì¶œí•˜ì—¬ ì¶”ì¶œëœ Featureë¥¼ ì–»ì–´ëƒ…ë‹ˆë‹¤. Recurrent LayerëŠ” ì¶”ì¶œëœ Featureì˜ ì „ì²´ì ì¸ Contextë¥¼ íŒŒì•…í•˜ê³  ë‹¤ì–‘í•œ outputì˜ í¬ê¸°ì— ëŒ€ì‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ëìœ¼ë¡œ Transcription layer(Fully connected layer)ëŠ” stepë§ˆë‹¤ ì–´ë–¤ characterì˜ í™•ë¥ ì´ ë†’ì€ì§€ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì•„ë˜ì˜ í‘œë¥¼ í†µí•´ ì •í™•í•œ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "[![](https://d3s0tskafalll9.cloudfront.net/media/original_images/e-23-3.crnn_structure.png)](https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "<br>\n",
        "\n",
        "ëª‡ ê°œì˜ Classê°€ í•„ìš”í•œì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•œ í›„ í€´ì¦ˆë¥¼ í’€ì–´ë³´ì„¸ìš”.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "h4XNGx0L1Mse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBERS = \"0123456789\"\n",
        "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
        "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
      ],
      "metadata": {
        "id": "T9VdqzRn2ret"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall structure of OCR\n",
        "\n",
        "<br>\n",
        "\n",
        "[![](https://d3s0tskafalll9.cloudfront.net/media/images/e-23-1.ocr-system.max-800x600.png)](https://brunch.co.kr/@kakao-it/318)\n",
        "\n",
        "<br>\n",
        "\n",
        "ìš°ë¦¬ê°€ ë§Œë“¤ê³ ì í•˜ëŠ” OCRì€ ì´ë¯¸ì§€ ì†ì—ì„œ ì˜ë¬¸ì„ Bounding boxë¡œ ì°¾ì•„ë‚´ê³  ê·¸ Bounding box ë‚´ì— ì–´ë–¤ Textê°€ í¬í•¨ë˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ì†ì—ì„œ ë¬¸ì ì˜ì—­ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì¸ **Text Detection**ì€ ì´ì „ì— ë´¤ë˜ ë°©ë²• ì¤‘ Segmentation ê¸°ë°˜ì˜ **CRAFT**ë¥¼ í™œìš©í•œ **keras-ocr**ì„ í™œìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤. **Recognition** ëª¨ë¸ì€ keras-ocrì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "\n",
        "* keras-ocr ê³µì‹ github\n",
        "\n",
        "* CRAFT: Character Region Awareness for Text detection\n",
        "\n",
        " * CRAFT Pytorch ê³µì‹ implementation\n",
        "\n",
        " * CRAFT Keras ë²„ì „ github\n",
        "\n",
        "<br>\n",
        "\n",
        ">keras-ocrì—ì„œë„ recognitionì„ ì§€ì›í•˜ëŠ”ë°ìš” ì´ ëª¨ë¸ì€ ì–´ë–¤ êµ¬ì¡°ë¥¼ ì¼ì„ê¹Œìš”?\n",
        "\n",
        ">Convolution layerì™€ RNNì„ ê²°í•©í•˜ê³  CTCë¡œ í•™ìŠµëœ CRNNì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "CNNê³¼ RNNì˜ ì•„ì´ë””ì–´ë¥¼ ê²°í•©í•˜ì—¬ Text Recognitionì˜ ì´ˆê¸° ëª¨ë¸ì˜ ë¼ˆëŒ€ë¥¼ ì™„ì„±í–ˆë˜ CRNN ëª¨ë¸ë„ 2015ë…„ì— ë‚˜ì˜¨ ê²ƒì…ë‹ˆë‹¤. ê·¸ ì´í›„ë¡œë„ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì´ ìƒˆë¡œìš´ ê¸°ë²•ì„ ì œì‹œí•˜ë©° ì¡°ê¸ˆì”© ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì–´ ì™”ìŠµë‹ˆë‹¤. 2019ë…„ì— ë°œí‘œëœ Naver Clovaì˜ ì•„ë˜ ë…¼ë¬¸ì—ì„œ ë‹¹ì‹œê¹Œì§€ì˜ ëª¨ë¸ì˜ ë°œì „ì‚¬ë¥¼ ì˜ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "* [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)\n",
        "\n",
        "<br>\n",
        "\n",
        ">keras-ocrì˜ CRNN ê¸°ë°˜ Recognition ëª¨ë¸ê³¼ ìœ„ ë…¼ë¬¸ì— ì†Œê°œëœ Recognitionì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ì–»ì€ (ì €ìë“¤ì˜) ëª¨ë¸ì€ ì–´ë–¤ ì ì´ ë‹¤ë¥¼ê¹Œìš”?\n",
        "\n",
        ">ì²« ë²ˆì§¸ë¡œ ì…ë ¥ ì´ë¯¸ì§€ ë³€í™˜ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì˜ ì•ì—ì„œ ê¸€ìë¥¼ Thin plate spline Transformationì„ í•´ì£¼ëŠ” TPS ëª¨ë“ˆì´ ë¶™ê³ , ë§ˆì§€ë§‰ Text ì¶œë ¥ ë‹¨ê³„ì—ì„œëŠ” Bidirectional LSTM ë’¤ë¡œ Attention decoderê°€ ë¶™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "DHYkL4toxlGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">ì˜ë¬¸ ëŒ€ë¬¸ìì™€ ìˆ«ìë¥¼ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œëŠ” ëª‡ê°€ì§€ì˜ Classê°€ í•„ìš”í• ê¹Œìš”?\n",
        "\n",
        ">ì´ 36ê°€ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë¬¸ìê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ì„œ ê³µë°±ì„ ì¶”ê°€í•  ê²½ìš° classì˜ ìˆ˜ëŠ” 37ê°œê°€ ë©ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "-fOaSCpt2wKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¨¼ì € lmdbë¥¼ ì´ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤. LMDBëŠ” Symasì—ì„œ ë§Œë“  Lightning Memory-Mapped Databaseì˜ ì•½ìì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ìš°ë¦¬ê°€ ë‹¤ë£¨ê²Œ ë  ë°ì´í„°ì…‹ì´ lmdb í¬ë§·(mdb)ì˜ íŒŒì¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "í´ë¼ìš°ë“œì—ëŠ” ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì•„ë˜ì˜ ëª…ë ¹ì–´ëŠ” ì°¸ê³ ë¡œ ì•Œì•„ë‘ì„¸ìš”.\n",
        "\n",
        "<br>\n",
        "\n",
        "<pre><code>$ pip install lmdb</code></pre>\n",
        "\n",
        "<br>\n",
        "\n",
        "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ importí•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œí•œ MJ ë°ì´í„°ì…‹ì˜ ìœ„ì¹˜ë„ í™•ì¸í•´ ì£¼ì„¸ìš”!\n",
        "\n"
      ],
      "metadata": {
        "id": "sndbWxH124Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import six\n",
        "import math\n",
        "import lmdb\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
        "\n",
        "TRAIN_DATA_PATH = HOME_DIR+'/data/MJ/MJ_train'\n",
        "VALID_DATA_PATH = HOME_DIR+'/data/MJ/MJ_valid'\n",
        "TEST_DATA_PATH = HOME_DIR+'/data/MJ/MJ_test'\n",
        "\n",
        "print(TRAIN_DATA_PATH)"
      ],
      "metadata": {
        "id": "o4jSyiel5dRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (2) Input Image\n",
        "\n",
        "<br>\n",
        "\n",
        "ë°ì´í„°ì…‹ ì•ˆì— ë“¤ì–´ìˆëŠ” ì´ë¯¸ì§€ê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤. ì•„ë˜ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ lmdbë¥¼ í†µí•´ í›ˆë ¨ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ë¥¼ 4ê°œë§Œ ì—´ì–´ì„œ ì‹¤ì œ shapeê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€, ì´ë¯¸ì§€ë‚˜ ë¼ë²¨ì€ ì–´ë–»ê²Œ ë‹¬ë ¤ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ ë³´ë„ë¡ í•©ì‹œë‹¤.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "h94294-U5iZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# envì— ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ê²Œìš”\n",
        "# lmdbì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œ envë¼ëŠ” ë³€ìˆ˜ëª…ì„ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¼ë°˜ì ì´ì—ìš”\n",
        "env = lmdb.open(TRAIN_DATA_PATH, \n",
        "                max_readers=32, \n",
        "                readonly=True, \n",
        "                lock=False, \n",
        "                readahead=False, \n",
        "                meminit=False)\n",
        "\n",
        "# ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ txn(transaction)ì´ë¼ëŠ” ë³€ìˆ˜ë¥¼ í†µí•´ ì—½ë‹ˆë‹¤\n",
        "# ì´ì œ txnë³€ìˆ˜ë¥¼ í†µí•´ ì§ì ‘ ë°ì´í„°ì— ì ‘ê·¼ í•  ìˆ˜ ìˆì–´ìš”\n",
        "with env.begin(write=False) as txn:\n",
        "    for index in range(1, 5):\n",
        "        # indexë¥¼ ì´ìš©í•´ì„œ ë¼ë²¨ í‚¤ì™€ ì´ë¯¸ì§€ í‚¤ë¥¼ ë§Œë“¤ë©´\n",
        "        # txnì—ì„œ ë¼ë²¨ê³¼ ì´ë¯¸ì§€ë¥¼ ì½ì–´ì˜¬ ìˆ˜ ìˆì–´ìš”\n",
        "        label_key = 'label-%09d'.encode() % index\n",
        "        label = txn.get(label_key).decode('utf-8')\n",
        "        img_key = 'image-%09d'.encode() % index\n",
        "        imgbuf = txn.get(img_key)\n",
        "        buf = six.BytesIO()\n",
        "        buf.write(imgbuf)\n",
        "        buf.seek(0)\n",
        "\n",
        "        # ì´ë¯¸ì§€ëŠ” ë²„í¼ë¥¼ í†µí•´ ì½ì–´ì˜¤ê¸° ë•Œë¬¸ì— \n",
        "        # ë²„í¼ì—ì„œ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ ë‹¤ì‹œ í•„ìš”í•´ìš”\n",
        "        try:\n",
        "            img = Image.open(buf).convert('RGB')\n",
        "\n",
        "        except IOError:\n",
        "            img = Image.new('RGB', (100, 32))\n",
        "            label = '-'\n",
        "\n",
        "        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¶œë ¥í•´ ë´…ë‹ˆë‹¤\n",
        "        width, height = img.size\n",
        "        print('original image width:{}, height:{}'.format(width, height))\n",
        "        \n",
        "        # ì´ë¯¸ì§€ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ë†’ì´ë¥¼ 32ë¡œ ë°”ê¿€ê±°ì—ìš”\n",
        "        # í•˜ì§€ë§Œ ë„ˆë¹„ë¥¼ 100ë³´ë‹¤ëŠ” ì‘ê²Œí•˜ê³  ì‹¶ì–´ìš”\n",
        "        target_width = min(int(width*32/height), 100)\n",
        "        target_img_size = (target_width,32)        \n",
        "        print('target_img_size:{}'.format(target_img_size))        \n",
        "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
        "\n",
        "        # ì´ì œ ë†’ì´ê°€ 32ë¡œ ì¼ì •í•œ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ í•¨ê»˜ ì¶œë ¥í•  ìˆ˜ ìˆì–´ìš”       \n",
        "        print('display img shape:{}'.format(img.shape))\n",
        "        print('label:{}'.format(label))\n",
        "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
      ],
      "metadata": {
        "id": "F0d3cD3o5ohT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì–´ë–¤ê°€ìš”? ëŒ€ë¶€ë¶„ì˜ ì´ë¯¸ì§€ì˜ heightëŠ” 31, ìµœëŒ€ 32ê¹Œì§€ë¡œ ë˜ì–´ ìˆê³ , widthëŠ” ë¬¸ìì—´ ê¸¸ì´ì— ë”°ë¼ ë‹¤ì–‘í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ì œë¶€í„° lmdbë¥¼ í™œìš©í•˜ì—¬ ì¼€ë¼ìŠ¤ ëª¨ë¸ í•™ìŠµìš© <code>MJSynth</code>ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. <code>dataset_path</code>ëŠ” ì½ì–´ë“¤ì¼ ë°ì´í„°ì…‹ì˜ ê²½ë¡œì…ë‹ˆë‹¤. <code>label_converter</code>ëŠ” ì•„ë˜ì—ì„œ ì—¬ëŸ¬ë¶„ì´ ë¬¸ìë¥¼ ë¯¸ë¦¬ì •ì˜ëœ indexë¡œ ë³€í™˜í•´ì£¼ëŠ” converterë¡œ ì§ì ‘ êµ¬í˜„í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ì™¸ì—ë„ <code>batch_size</code>ì™€ ì…ë ¥ì´ë¯¸ì§€ í¬ê¸° ê·¸ë¦¬ê³  í•„í„°ë§ì„ ìœ„í•œ ìµœëŒ€ ê¸€ì ìˆ˜, í•™ìŠµëŒ€ìƒìœ¼ë¡œ í•œì •í•˜ê¸° ìœ„í•œ characterë“±ì„ ì…ë ¥ìœ¼ë¡œ ë°›ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "mRqg8lDK5sdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MJDatasetSequence(Sequence):\n",
        "    # ê°ì²´ë¥¼ ì´ˆê¸°í™” í•  ë•Œ lmdbë¥¼ ì—´ì–´ envì— ì¤€ë¹„í•´ë‘¡ë‹ˆë‹¤\n",
        "    # ë˜, lmdbì— ìˆëŠ” ë°ì´í„° ìˆ˜ë¥¼ ë¯¸ë¦¬ íŒŒì•…í•´ë‘¡ë‹ˆë‹¤\n",
        "    def __init__(self, \n",
        "                 dataset_path,\n",
        "                 label_converter,\n",
        "                 batch_size=1,\n",
        "                 img_size=(100,32),\n",
        "                 max_text_len=22,\n",
        "                 is_train=False,\n",
        "                 character='') :\n",
        "        \n",
        "        self.label_converter = label_converter\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.max_text_len = max_text_len\n",
        "        self.character = character\n",
        "        self.is_train = is_train\n",
        "        self.divide_length = 100\n",
        "\n",
        "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            self.num_samples = int(txn.get('num-samples'.encode()))\n",
        "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
        "    \n",
        "    # indexì— í•´ë‹¹í•˜ëŠ” imageì™€ labelì„ ì½ì–´ì˜µë‹ˆë‹¤\n",
        "    # ìœ„ì—ì„œ ì‚¬ìš©í•œ ì½”ë“œì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤\n",
        "    # labelì„ ì¡°ê¸ˆ ë” ë‹¤ë“¬ëŠ” ê²ƒì´ ì•½ê°„ ë‹¤ë¦…ë‹ˆë‹¤\n",
        "    def _get_img_label(self, index):\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            label_key = 'label-%09d'.encode() % index\n",
        "            label = txn.get(label_key).decode('utf-8')\n",
        "            img_key = 'image-%09d'.encode() % index\n",
        "            imgbuf = txn.get(img_key)\n",
        "\n",
        "            buf = six.BytesIO()\n",
        "            buf.write(imgbuf)\n",
        "            buf.seek(0)\n",
        "            try:\n",
        "                img = Image.open(buf).convert('RGB')\n",
        "\n",
        "            except IOError:\n",
        "                img = Image.new('RGB', self.img_size)\n",
        "                label = '-'\n",
        "            width, height = img.size\n",
        "            \n",
        "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
        "            target_img_size = (target_width, self.img_size[1])\n",
        "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
        "            # labelì„ ì•½ê°„ ë” ë‹¤ë“¬ìŠµë‹ˆë‹¤\n",
        "            label = label.upper()\n",
        "            out_of_char = f'[^{self.character}]'\n",
        "            label = re.sub(out_of_char, '', label)\n",
        "            label = label[:self.max_text_len]\n",
        "\n",
        "        return (img, label)\n",
        "    \n",
        "    # __getitem__ì€ ì•½ì†ë˜ì–´ìˆëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤\n",
        "    # ì´ ë¶€ë¶„ì„ ì‘ì„±í•˜ë©´ sliceí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "    # ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”\n",
        "    # https://docs.python.org/3/reference/datamodel.html#object.__getitem__\n",
        "    # \n",
        "    # 1. idxì— í•´ë‹¹í•˜ëŠ” index_listë§Œí¼ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬\n",
        "    # 2. imageì™€ labelì„ ë¶ˆëŸ¬ì˜¤ê³  \n",
        "    # 3. ì‚¬ìš©í•˜ê¸° ì¢‹ì€ inputsê³¼ outputsí˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤\n",
        "    def __getitem__(self, idx):\n",
        "        # 1.\n",
        "        batch_indicies = self.index_list[\n",
        "            idx*self.batch_size:\n",
        "            (idx+1)*self.batch_size\n",
        "        ]\n",
        "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
        "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
        "\n",
        "        input_length = np.ones([self.batch_size], dtype='int64') * self.max_text_len\n",
        "        label_length = np.ones([self.batch_size], dtype='int64')\n",
        "\n",
        "        # 2.\n",
        "        for i, index in enumerate(batch_indicies):\n",
        "            img, label = self._get_img_label(index)\n",
        "            encoded_label = self.label_converter.encode(label)\n",
        "            # ì¸ì½”ë”© ê³¼ì •ì—ì„œ '-'ì´ ì¶”ê°€ë˜ë©´ max_text_lenë³´ë‹¤ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆì–´ìš”\n",
        "            if len(encoded_label) > self.max_text_len:\n",
        "                continue\n",
        "            width = img.shape[0]\n",
        "            input_images[i,:width,:,:] = img\n",
        "            labels[i,0:len(encoded_label)] = encoded_label\n",
        "            label_length[i] = len(encoded_label)\n",
        "        \n",
        "        # 3.\n",
        "        inputs = {\n",
        "            'input_image': input_images,\n",
        "            'label': labels,\n",
        "            'input_length': input_length,\n",
        "            'label_length': label_length,\n",
        "        }\n",
        "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
        "\n",
        "        return inputs, outputs"
      ],
      "metadata": {
        "id": "C00QKX245v1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ„ì˜ ë¶„ì„ì½”ë“œì²˜ëŸ¼ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ img, labelì˜ ìŒìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ì€ <code>_get_img_label()</code> ë©”ì†Œë“œì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  <code>model.fit()</code>ì—ì„œ í˜¸ì¶œë˜ëŠ” <code>__getitem__()</code> ë©”ì†Œë“œì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë§Œí¼ <code>_get_img_label()</code> ë¥¼ í†µí•´ ê°€ì ¸ì˜¨ ë°ì´í„°ì…‹ì„ ë¦¬í„´í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. <code>_get_img_label()</code> ë¥¼ ë³´ë©´ ë‹¤ì–‘í•œ ì‚¬ì´ì¦ˆì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ heightëŠ” 32ë¡œ ë§ì¶”ê³ , widthëŠ” ìµœëŒ€ 100ê¹Œì§€ë¡œ ë§ì¶”ê²Œë” ê°€ê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "WJiEuWGT6KkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (3) Encode\n",
        "\n",
        "<br>\n",
        "\n",
        "ì´ì „ ìŠ¤í…ì—ì„œ ì‚´í´ë³¸ ë°”ì— ì˜í•˜ë©´, Labelì´ ìš°ë¦¬ê°€ ì½ì„ ìˆ˜ ìˆëŠ” í‰ë¬¸ Textë¡œ ì´ë£¨ì–´ì ¸ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ ì ì ˆí•œ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤. ë”°ë¼ì„œ ê° Characterë¥¼ classë¡œ ìƒê°í•˜ê³  ì´ë¥¼ stepì— ë”°ë¥¸ class indexë¡œ ë³€í™˜í•´ì„œ encodeë¥¼ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ì¤„ ìˆ˜ ìˆëŠ” <code>LabelConverter</code> í´ë˜ìŠ¤ë¥¼ ì‘ì„±í•´ ë´…ì‹œë‹¤.\n",
        "\n",
        "* <code>__init__()</code>ì—ì„œëŠ” ì…ë ¥ìœ¼ë¡œ ë°›ì€ textë¥¼ <code>self.dict</code>ì— ê° characterë“¤ì´ ì–´ë–¤ indexì— ë§¤í•‘ë˜ëŠ”ì§€ ì €ì¥í•©ë‹ˆë‹¤. ì´ characterì™€ index ì •ë³´ë¥¼ í†µí•´ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” outputì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ë§Œì•½ <code>character='ABCD'</code>ë¼ë©´ <code>'A'</code>ì˜ labelì€ 1, <code>'B'</code>ì˜ labelì€ 2ê°€ ë©ë‹ˆë‹¤.\n",
        "* ê³µë°±(blank) ë¬¸ìë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê³µë°± ë¬¸ìë¥¼ ëœ»í•˜ê¸° ìœ„í•´ <code>'-'</code>ë¥¼ í™œìš©í•˜ë©°, labelì€ 0ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "* <code>decode()</code>ëŠ” ê° indexë¥¼ ë‹¤ì‹œ characterë¡œ ë³€í™˜í•œ í›„ ì´ì–´ì£¼ì–´ ìš°ë¦¬ê°€ ì½ì„ ìˆ˜ ìˆëŠ” textë¡œ ë°”ê¾¸ì–´ì¤ë‹ˆë‹¤.\n",
        "\n",
        ">ì…ë ¥ë°›ì€ textë¥¼ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” labelë¡œ ë§Œë“œëŠ” encode() ë©”ì†Œë“œë¥¼ êµ¬í˜„í•´ ì£¼ì„¸ìš”!\n",
        "ë‹¨, ê°™ì€ ê¸€ìê°€ ì—°ì†ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ê²½ìš°ì—ëŠ” ì´ì–´ì§€ëŠ” ê·¸ ì‚¬ì´ì— ê³µë°± ë¬¸ìì˜ labelì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤!\n",
        "\n",
        "OCR ëª¨ë¸ í•™ìŠµë°ì´í„°ì— ì™œ ê³µë°± ë¬¸ìê°€ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ”ì§€ëŠ” ë‹¤ìŒ ìŠ¤í…ì—ì„œ ì„¤ëª…í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "UkP9oh_m6ikU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelConverter(object):\n",
        "\n",
        "     def __init__(self, character):\n",
        "         self.character = \"-\" + character\n",
        "         self.label_map = dict()\n",
        "         for i, char in enumerate(self.character):\n",
        "             self.label_map[char] = i\n",
        "\n",
        "     def encode(self, text):\n",
        "         encoded_label = []\n",
        "         for i, char in enumerate(text):\n",
        "             if i > 0 and char == text[i - 1]:\n",
        "                 encoded_label.append(0)    # ê°™ì€ ë¬¸ì ì‚¬ì´ì— ê³µë°± ë¬¸ì labelì„ ì‚½ì…\n",
        "             encoded_label.append(self.label_map[char])\n",
        "         return np.array(encoded_label)\n",
        "\n",
        "     def decode(self, encoded_label):\n",
        "         target_characters = list(self.character)\n",
        "         decoded_label = \"\"\n",
        "         for encode in encoded_label:\n",
        "             decoded_label += self.character[encode]\n",
        "         return decoded_label"
      ],
      "metadata": {
        "id": "ZbzNa4nR7Qm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•„ë˜ì—ì„œ 'HELLO'ë¥¼ Encodeí•œ í›„ Decodeê°€ ì •ìƒì ìœ¼ë¡œ ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ë„ë¡ í•´ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "bphJS2eP7cyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
        "\n",
        "encdoded_text = label_converter.encode('HELLO')\n",
        "print(\"Encdoded_text: \", encdoded_text)\n",
        "decoded_text = label_converter.decode(encdoded_text)\n",
        "print(\"Decoded_text: \", decoded_text)"
      ],
      "metadata": {
        "id": "XwRWQJ7J7grs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë™ì¼í•œ ê¸€ì <code>'L'</code>ì´ ì—°ì†ë  ë•Œ, ê·¸ ì‚¬ì´ì— ê³µë°± ë¬¸ìê°€ í¬í•¨ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "QnIdoo2P7inJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (4) Build CRNN model\n",
        "\n",
        "<br>\n",
        "\n",
        "ì´ì œ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì¤€ë¹„í–ˆìœ¼ë‹ˆ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³¼ ì°¨ë¡€ì…ë‹ˆë‹¤. Kerasì—ì„œ ì œê³µí•˜ëŠ” <code>K.ctc_batch_cost()</code>ë¥¼ í™œìš©í•´ì„œ lossë¥¼ ê³„ì‚°í•˜ë„ë¡ <code>ctc_lambda_func</code>ë¥¼ ì•„ë˜ì™€ ê°™ì´ ë§Œë“¤ì–´ë‘ì—ˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "DEQ7z0OO7oHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_lambda_func(args): # CTC lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ Lambda í•¨ìˆ˜\n",
        "    labels, y_pred, label_length, input_length = args\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "metadata": {
        "id": "XEcy_93AFnA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì—¬ê¸°ì„œ ì ì‹œ Kerasì˜ <code>K.ctc_batch_cost()</code> í•¨ìˆ˜ì— ëŒ€í•´ ì§šê³  ë„˜ì–´ê°‘ì‹œë‹¤. ë¹„ë¡ ìš°ë¦¬ê°€ ì´ í•¨ìˆ˜ ë‚´ë¶€ë¥¼ ì§ì ‘ êµ¬í˜„í•˜ì§„ ì•Šê² ì§€ë§Œ CTC Loss í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ìš°ë¦¬ê°€ ì´ í•¨ìˆ˜ì— ì¸ìë¡œ ì–´ë–¤ ê°’ì„ ë„˜ê²¨ì•¼ í•˜ëŠ”ì§€ëŠ” ëª…í™•í•˜ê²Œ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "* [Tensorflow Tutorial - ctc_batch_cost](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/backend/ctc_batch_cost)\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/ctc.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "ìš°ë¦¬ëŠ” CTC Lossë¥¼ í™œìš©í•´ì•¼ í•˜ëŠ” ëª¨ë¸ì´ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ ìƒí™©ì„ ë‹¤ë£¨ê¸° ìœ„í•œ ê²ƒì„ì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì…ë ¥ì˜ ê¸¸ì´ Tì™€ ë¼ë²¨ì˜ ê¸¸ì´ Uì˜ ë‹¨ìœ„ê°€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ë•Œ, ê·¸ë˜ì„œ ë¼ë²¨ì€ <code>APPLE</code>ì´ì§€ë§Œ ëª¨ë¸ì´ ì¶œë ¥í•œ ê²°ê³¼ëŠ” <code>AAAPPPPLLLLEE</code> ì²˜ëŸ¼ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒí™©ì´ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë¼ë²¨ì„ ì¶”ë¡ í•´ì•¼ í•˜ëŠ” Text recognition íƒœìŠ¤í¬ì— ë™ì¼í•˜ê²Œ ì ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "\n",
        ">ë§Œì•½ ëª¨ë¸ì´ AAAPPPPLLLLEEì„ ì¶œë ¥í–ˆë‹¤ê³  í•©ì‹œë‹¤. ì´ë•Œ ì¶”ë¡  ê²°ê³¼ëŠ” APLEì¼ì§€ APPLEì¼ì§€ êµ¬ë¶„ì´ ê°€ëŠ¥í• ê¹Œìš”? ì´ ê²½ìš°ì—ëŠ” APLEë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n",
        ">\n",
        ">ê·¸ëŸ¬ë¯€ë¡œ ì¶”ë¡  ê²°ê³¼ê°€ APPLEì´ ë˜ê²Œ í•˜ë ¤ë©´ ì´ë¯¸ì§€ì˜ ë¼ë²¨ì€ AP-PLEë¡œ ë³´ì •í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ëª¨ë¸ì´ AAAPP-PPLLLEEë¡œ ì¶œë ¥ì„ í•œë‹¤ë©´ ì¶”ë¡  ê²°ê³¼ëŠ” APPLEì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŸ° ì´ìœ ë¡œ ì´ì „ ìŠ¤í…ì—ì„œ LabelConverter.encode() ë©”ì†Œë“œì— ê³µë°±ë¬¸ì ì²˜ë¦¬ë¡œì§ì„ í¬í•¨í–ˆë˜ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "\n",
        "ìœ„ í…ì„œí”Œë¡œìš° íŠœí† ë¦¬ì–¼ì— ë”°ë¥´ë©´, <code>K.ctc_batch_cost(y_true, y_pred, input_length, label_length)</code>ì—ëŠ” 4ê°€ì§€ ì¸ìê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ê°ê°ì˜ ì¸ìì˜ ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "* y_true: tensor (samples, max_string_length) containing the truth labels.\n",
        "* y_pred: tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.\n",
        "* input_length tensor: (samples, 1) containing the sequence length for each batch item in y_pred.\n",
        "* label_length tensor: (samples, 1) containing the sequence length for each batch item in y_true.\n",
        "\n",
        "(ì—¬ê¸°ì„œ samplesëŠ” ë°°ì¹˜ì‚¬ì´ì¦ˆë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.)\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-6-P-example.png)\n",
        "\n",
        "ìœ„ ê·¸ë¦¼ì€ ì´ì „ ìŠ¤í…ì—ì„œ ì‚´í´ë³¸ ì‹¤ì œ ë°ì´í„°ì…‹ ì˜ˆì‹œì…ë‹ˆë‹¤. ì´ ì¼€ì´ìŠ¤ë¥¼ ì˜ˆë¡œ ë“¤ì—ˆì„ ë•Œ ìœ„ ì¸ìë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ë  ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "* <code>y_true</code>: ì‹¤ì œ ë¼ë²¨ <code>LUBE</code>. í…ìŠ¤íŠ¸ ë¼ë²¨ ê·¸ëŒ€ë¡œê°€ ì•„ë‹ˆë¼, ê° ê¸€ìë¥¼ One-hot ì¸ì½”ë”©í•œ í˜•íƒœë¡œì„œ, max_string_length ê°’ì€ ëª¨ë¸ì—ì„œ 22ë¡œ ì§€ì •í•  ì˜ˆì •\n",
        "* <code>y_pred</code>: ìš°ë¦¬ê°€ ë§Œë“¤ RCNN <code>ëª¨ë¸ì˜ ì¶œë ¥ ê²°ê³¼</code>. ê¸¸ì´ëŠ” 4ê°€ ì•„ë‹ˆë¼ ìš°ë¦¬ê°€ ë§Œë“¤ RNNì˜ ìµœì¢… ì¶œë ¥ ê¸¸ì´ë¡œì„œ 24ê°€ ë  ì˜ˆì •\n",
        "* <code>input_length tensor</code>: ëª¨ë¸ ì…ë ¥ ê¸¸ì´ Të¡œì„œ, ì´ ê²½ìš°ì—ëŠ” í…ìŠ¤íŠ¸ì˜ widthì¸ <code>74</code>\n",
        "* <code>label_length tensor</code>: ë¼ë²¨ì˜ ì‹¤ì œ ì •ë‹µ ê¸¸ì´ Uë¡œì„œ, ì´ ê²½ìš°ì—ëŠ” <code>4</code>\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "ì´ì œ, <code>K.ctc_batch_cost()</code>ë¥¼ í™œìš©í•˜ì—¬, <code>image_input</code>ì„ ì…ë ¥ìœ¼ë¡œ, ë§ˆì§€ë§‰ Labelì„ 'output'ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ë ˆì´ì–´ë¥¼ ê°–ë„ë¡ ëª¨ë¸ì„ ë§Œë“œëŠ” í•¨ìˆ˜ <code>build_crnn_model()</code>ì„ êµ¬í˜„í•´ ë´…ì‹œë‹¤.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "JEcDGZ-7Foje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
        "    num_chars = len(characters)+2\n",
        "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
        "    \n",
        "    # Build CRNN model\n",
        "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
        "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
        "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.BatchNormalization()(conv)\n",
        "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = layers.BatchNormalization()(conv)\n",
        "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
        "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
        "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
        "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
        "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
        "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
        "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
        "\n",
        "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
        "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
        "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
        "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
        "        [labels, y_pred, label_length, input_length]\n",
        "    )\n",
        "    model_input = [image_input, labels, input_length, label_length]\n",
        "    model = Model(\n",
        "        inputs=model_input,\n",
        "        outputs=loss_out\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "a9X1FpuDH8uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognition model (5) Train & Inference\n",
        "\n",
        "<br>\n",
        "\n",
        "ì´ì œ ì•ì—ì„œ ì •ì˜í•œ <code>MJDatasetSequence</code>ë¡œ ë°ì´í„°ë¥¼ ì ì ˆíˆ ë¶„ë¦¬í•˜ì—¬ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ í†µí•´ í•™ìŠµì„ ì‹œì¼œë´…ì‹œë‹¤.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "B_C67gcJIA9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤\n",
        "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
        "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
        "model = build_crnn_model()\n",
        "\n",
        "# ëª¨ë¸ì„ ì»´íŒŒì¼ í•©ë‹ˆë‹¤\n",
        "optimizer = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "_QGHVizHIJRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„  ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ë‹ˆ ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ 1 Epochë§Œ ëŒë ¤ë³´ê² ìŠµë‹ˆë‹¤. ë’¤ì—ì„œëŠ” 20Epoch ì´ìƒ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì§„í–‰í• ê²Œìš”.ğŸ˜˜\n",
        "\n",
        "EarlyStoppingì„ ì´ìš©í•˜ë©´ í›ˆë ¨ì´ ë” ë¹¨ë¦¬ ëë‚  ìˆ˜ë„ ìˆì–´ìš”!\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "VBbJFusnILQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í›ˆë ¨ì´ ë¹¨ë¦¬ ëë‚  ìˆ˜ ìˆë„ë¡ ModelCheckPointì™€ EarlyStoppingì„ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
        "checkpoint_path = HOME_DIR + '/model_checkpoint.hdf5'\n",
        "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_loss',\n",
        "    verbose=1, save_best_only=True, save_weights_only=True\n",
        ")\n",
        "earlystop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='min'\n",
        ")\n",
        "model.fit(train_set,\n",
        "          steps_per_epoch=len(train_set),\n",
        "          epochs=1,\n",
        "          validation_data=val_set,\n",
        "          validation_steps=len(val_set),\n",
        "          callbacks=[ckp, earlystop])"
      ],
      "metadata": {
        "id": "iWKDRb2MIOW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì œ í•™ìŠµëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ì…‹ì„ í†µí•´ í™•ì¸í•´ ë³¼ ì°¨ë¡€ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Mt7Xe1a8IQr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‹¤ìŒì€ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ì €ì¥ëœ ê²½ë¡œì…ë‹ˆë‹¤\n",
        "checkpoint_path = HOME_DIR + '/data/model_checkpoint.hdf5'\n",
        "\n",
        "# ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
        "test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
        "model = build_crnn_model()\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# crnn ëª¨ë¸ì€ ì…ë ¥ì´ ë³µì¡í•œ êµ¬ì¡°ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤\n",
        "# ê·¸ë˜ì„œ crnn ëª¨ë¸ì˜ ì…ë ¥ì¤‘ 'input_image' ë¶€ë¶„ë§Œ ì‚¬ìš©í•œ ëª¨ë¸ì„ ìƒˆë¡œ ë§Œë“¤ê²ë‹ˆë‹¤\n",
        "# inference ì „ìš© ëª¨ë¸ì´ì—ìš” \n",
        "input_data = model.get_layer('input_image').output\n",
        "y_pred = model.get_layer('output').output\n",
        "model_pred = Model(inputs=input_data, outputs=y_pred)"
      ],
      "metadata": {
        "id": "6AgG0ymBIVJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëˆˆìœ¼ë¡œ í™•ì¸í•´ë´…ì‹œë‹¤."
      ],
      "metadata": {
        "id": "V3Uhh6Q9Ii0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# ëª¨ë¸ì´ inferenceí•œ ê²°ê³¼ë¥¼ ê¸€ìë¡œ ë°”ê¿”ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤\n",
        "# ì½”ë“œ í•˜ë‚˜í•˜ë‚˜ë¥¼ ì´í•´í•˜ê¸°ëŠ” ì¡°ê¸ˆ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "def decode_predict_ctc(out, chars = TARGET_CHARACTERS):\n",
        "    results = []\n",
        "    indexes = K.get_value(\n",
        "        K.ctc_decode(\n",
        "            out, input_length=np.ones(out.shape[0]) * out.shape[1],\n",
        "            greedy=False , beam_width=5, top_paths=1\n",
        "        )[0][0]\n",
        "    )[0]\n",
        "    text = \"\"\n",
        "    for index in indexes:\n",
        "        text += chars[index]\n",
        "    results.append(text)\n",
        "    return results\n",
        "\n",
        "# ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì´ ì£¼ì–´ì§€ë©´ inferenceë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤\n",
        "# indexê°œ ë§Œí¼ì˜ ë°ì´í„°ë¥¼ ì½ì–´ ëª¨ë¸ë¡œ inferenceë¥¼ ìˆ˜í–‰í•˜ê³ \n",
        "# ê²°ê³¼ë¥¼ ë””ì½”ë”©í•´ ì¶œë ¥í•´ì¤ë‹ˆë‹¤\n",
        "def check_inference(model, dataset, index = 5):\n",
        "    for i in range(index):\n",
        "        inputs, outputs = dataset[i]\n",
        "        img = dataset[i][0]['input_image'][0:1,:,:,:]\n",
        "        output = model.predict(img)\n",
        "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
        "        print(\"Result: \\t\", result)\n",
        "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))\n",
        "\n",
        "check_inference(model_pred, test_set, index=10)"
      ],
      "metadata": {
        "id": "kmpSOgvfItRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í”„ë¡œì íŠ¸: End-to-End OCR"
      ],
      "metadata": {
        "id": "RgoEEc2HpURT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CeVQ-waepW4t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}